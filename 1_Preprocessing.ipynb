{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mTRF Pre-processing\n",
    "\n",
    "Version 4 路 Last update: 09/10/2024\n",
    "\n",
    "## Overview\n",
    "\n",
    "The mTRF Pre-processing consists in the following steps, and takes ~1 hr per session:\n",
    "* [Setup the environment](#setup)\n",
    "* [Load the data](#load) (~2 min)\n",
    "* [Bad channel detection](#bad) (~10 min)\n",
    "* [Maxwell filtering and Signal-space separation](#maxwell) (~15 min) 路 <span style=\"color:#ffcc00\">Checkpoint #1</span>\n",
    "* [Generate the ICA](#ica) (~15 min) 路 <span style=\"color:#ffcc00\">Checkpoint #2</span>\n",
    "* [Remove the heartbeat and eye movements](#remove) (~10 min) 路 <span style=\"color:#ffcc00\">Checkpoint #3</span>\n",
    "* [Pre-process the empty room recording](#empty) (~5 min)\n",
    "\n",
    "Then, the freesurfer steps take more time:\n",
    "* [Cortical reconstruction with Freesurfer](#freesurfer) (~ 8 hr)\n",
    "* [Co-registration](#coregistration) (~20 min)\n",
    "\n",
    "Processing part (in development):\n",
    "* [Normalize the conditions](#triggers) (~5 min)\n",
    "\n",
    "Approx. times are indicative and can vary depending on the length of the processed data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Setup\n",
    "\n",
    "### Access Cajal\n",
    "\n",
    "* From Citrix (remotely):\n",
    "\n",
    "    Go to https://gateway.bcbl.eu/Citrix/BCBL_GatewayWeb/ and open **Terminator** (Linux Apps).\n",
    "    \n",
    "    Once on your virtual desktop, connect to cajal03: `vglconnect USERNAME@cajal03 -Y`, and enter your password.\n",
    "\n",
    "* From a BCBL computer (locally):\n",
    "    \n",
    "    In the Start menu, open Cygwin-X > run XWin Server (if you cannot find it on your BCBL computer, open a ticket to install it).\n",
    "\n",
    "    Two X icons will appear in the system tray. Right click on the icon with the green X > System tools > XTerm. These steps are necessary to obtain a GUI (running the following line on the regular Windows terminal will not work).\n",
    "    \n",
    "    Once on it, type in `ssh cajal03 -Y`, followed by your password."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the environment\n",
    "Load **Python** using `module load python`.\n",
    "\n",
    "Run **Visual Studio Code** using `code`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "Be sure to run the code on an environment with the required Python modules installed: **numpy**, **matplotlib**, **mne** and **mne_bids**.\n",
    "\n",
    "To install a Python module, use `py -m pip install MODULENAME` or `python -m pip install MODULENAME`. Note that to install **mne_bids**, the MODULENAME will be `mne-bids`.\n",
    "\n",
    "The environment **megtrain (Python 3.11.6)** should contain the required modules.\n",
    "(to modify it, `conda activate /bcbl/home/public/MEGtrain/Code/megtrain/megtrain/.conda`)\n",
    "\n",
    "(modules needed in the venv: `numpy`, `matplotlib`, `mne`, `mne-bids`, `ipywidgets`, `ipyevents`, `nibabel`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import\"></a>\n",
    "### Import sections\n",
    "*Approx. time: 1 s*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate in the OS and call folders\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "# Date and time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# Get the username automatically\n",
    "import getpass\n",
    "\n",
    "# Perform plots\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "\n",
    "# MEG processing\n",
    "import mne\n",
    "from mne_bids import (\n",
    "    write_raw_bids,\n",
    "    read_raw_bids,\n",
    "    BIDSPath,\n",
    "    write_meg_calibration,\n",
    "    write_meg_crosstalk,\n",
    "    mark_channels\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## Load the data\n",
    "\n",
    "### Locate the subject data and define output path\n",
    "*Approx. time: 1 s*\n",
    "\n",
    "<span style=\"color:red\">Contains a prompt asking the user to enter one of the subjects</span>, with the path automatically taken from the dictionary \"subjects\".\n",
    "\n",
    "<span style=\"color:#99cc00\">Creates: ``Derivatives/Preprocessing/sub-XX`` folder.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "user = getpass.getuser()\n",
    "path_data = op.join('/export/home', user, 'lab/MEG_EXPERIMENTS/EXPERIMENT_NAME/DATA/FIF/')\n",
    "\n",
    "# Subjects: Write down the path to the subjects of your experiment here. \n",
    "# The path should contain the name of the first FIF file (without the .fif extension).\n",
    "subjects = {\"subject_01a\": 'path_to_subject_01/session_01/block_01',\n",
    "            \"subject_01b\": 'path_to_subject_01/session_02/block_01',\n",
    "            \"subject_02a\": 'path_to_subject_02/session_01/block_01',\n",
    "            \"subject_02b\": 'path_to_subject_02/session_02/block_01'}\n",
    "\n",
    "subject = input(\"Subject name? \")\n",
    "if subject in subjects.keys():\n",
    "    subject_fif = subjects[subject] + \".fif\"\n",
    "    subject_number = subject[8:10]\n",
    "    subject_session = \"02\" if subjects[subject].split(\"/\")[0][-2:] == \"_2\" else \"01\"\n",
    "    subject_eelbrain_code = \"sub-\" + subject_number\n",
    "    visit = \"6mt\" if 'b' in subjects[subject].split(\"_\")[1] else \"\"\n",
    "else:\n",
    "    raise Exception(\"Subject name not found.\")\n",
    "\n",
    "# Subject path\n",
    "raw_fif_name = op.join(path_data, subject_fif)\n",
    "print(f\"Path of the selected subject: {str(raw_fif_name)}\")\n",
    "\n",
    "# Define the raw BIDS output path\n",
    "path_output = op.join('/export/home', user, 'public/MEGtrain/')\n",
    "path_bids = op.join(path_output, 'bids')\n",
    "\n",
    "# Calibration and crosstalk files \n",
    "fine_cal_file = op.join(path_output,'scripts/maxfilter_parameters/sss_cal_3049.dat')\n",
    "crosstalk_file = op.join(path_output,'scripts/maxfilter_parameters/ct_sparse.fif')\n",
    "\n",
    "# Define the preprocessing output path\n",
    "path_preprocessing = op.join(path_output, \"derivatives/Preprocessing/\")\n",
    "\n",
    "path_derivatives = BIDSPath(\n",
    "    subject = subject_number, \n",
    "    session = subject_session, \n",
    "    task = 'track', \n",
    "    run = \"01\",\n",
    "    suffix = \"meg\",\n",
    "    root = path_preprocessing\n",
    ")\n",
    "os.makedirs(path_derivatives.directory, exist_ok = True)\n",
    "\n",
    "print(f\"User: {user}\")\n",
    "print(f\"Subject: {subject}\")\n",
    "print(f\"Subject number: {subject_number}\")\n",
    "print(f\"Subject session: {subject_session}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the output MNE path\n",
    "*Approx. time: 1 m 30 s*\n",
    "\n",
    "Reads the original FIF files and uniformizes the data by creating a BIDS structure in the path `path_output`. We also create the MEG fine calibration and cross-talk files, which are MEG files that are specific to the MEG acquistion machine, and describe its parameters. They allow to fine-tune the Maxwell filtering later on.\n",
    "\n",
    "<span style=\"color:#99cc00\">Creates: ``MEGtrain/sub-XX/ses-XX/`` folder, with:</span>\n",
    "\n",
    "* ``sub-XX_ses-XX_scans.tsv``\n",
    "* ``meg/sub-XX_ses-XX_coordsystem.json``\n",
    "* ``meg/sub-XX_ses-XX_task-track_run-01_channels.tsv``\n",
    "* ``meg/sub-XX_ses-XX_task-track_run-01_meg.json``\n",
    "* ``meg/sub-XX_ses-XX_task-track_run-01_split-XX_meg.fif`` <span style=\"color:#99cc00\">(multiple files)</span>\n",
    "* ``meg/sub-XX_ses-XX_task-track_acq-calibration_meg.dat``\n",
    "* ``meg/sub-XX_ses-XX_task-track_acq-crosstalk_meg.fif``\n",
    "\n",
    "<span style=\"color:#99cc00\">The files ``dataset_description.json``, ``participants.json``, and ``participants.tsv`` are also created or updated.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the FIF file\n",
    "raw_fif = mne.io.read_raw_fif(raw_fif_name, preload = False)\n",
    "\n",
    "# Define the BIDS path\n",
    "path_bids_raw = BIDSPath(\n",
    "    subject = subject_number, \n",
    "    session = subject_session, \n",
    "    task = 'track', \n",
    "    run = \"01\",\n",
    "    datatype = \"meg\", \n",
    "    root = path_bids)\n",
    "\n",
    "# Write the BIDS files\n",
    "write_raw_bids(raw = raw_fif, bids_path = path_bids_raw, overwrite = True)\n",
    "\n",
    "# Write calibration files in BIDS format\n",
    "write_meg_calibration(fine_cal_file, path_bids_raw)\n",
    "write_meg_crosstalk(crosstalk_file, path_bids_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bad\"></a>\n",
    "## Bad channel detection\n",
    "### Automatic detection\n",
    "*Approx. time: 5 m*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the BIDS data\n",
    "raw_bids = read_raw_bids(path_bids_raw)\n",
    "\n",
    "# Define the \"bads\" channels to being empty\n",
    "raw_bids.info['bads'] = []\n",
    "\n",
    "# Run the automatic detection\n",
    "auto_noisy_channels, auto_flat_channels, auto_scores = mne.preprocessing.find_bad_channels_maxwell(\n",
    "    raw_bids,  # Raw data to process\n",
    "    cross_talk = crosstalk_file, # MEG crosstalk file\n",
    "    calibration = fine_cal_file,  # MEG fine-calibration file\n",
    "    return_scores = True, \n",
    "    duration = 30,  # Duration of each window (in seconds)\n",
    "    min_count = 10,  # Number of window appearances to be counted as bad channel\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Flat channels: {str(auto_flat_channels)}\")\n",
    "print(f\"Bad channels: {str(auto_noisy_channels)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual detection\n",
    "\n",
    "Check the channels visually, to check if some channels weren't detected as bad automatically, or if there were false positives. \n",
    "\n",
    "Tips: \"Zooming out\" allows to detect noisy (bad) channels, while \"Zooming in\" allows to detect flat channels.\n",
    "\n",
    "Click on the name of a channel in the left part of the screen to mark them as bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_bids.plot(lowpass = 30, highpass = 1, overview_mode = \"hidden\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write the bad channels in the dictionary below to preserve a trace of them and add them to the bad channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually writing the bad channels\n",
    "bad_channels_manual = {\"subject_01a\": [\"MEG2413\", \"MEG2623\"],\n",
    "                       \"subject_01b\": [\"MEG1912\"],\n",
    "                       \"subject_02a\": [\"MEG2623\"],\n",
    "                       \"subject_02b\": [\"MEG0312\", \"MEG0711\"]}\n",
    "\n",
    "raw_bids.info[\"bads\"] = bad_channels_manual[subject]\n",
    "\n",
    "# Combining the automatically found bad and flat channels to the manual ones\n",
    "raw_bids.info[\"bads\"] += auto_noisy_channels + auto_flat_channels\n",
    "\n",
    "print(f\"Bad channels: {str(raw_bids.info['bads'])}\")\n",
    "\n",
    "# Once the bad channels are defined, we save them in the file itself\n",
    "mark_channels(path_bids_raw, ch_names = raw_bids.info['bads'], status = \"bad\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"maxwell\"></a>\n",
    "## Maxwell filtering and Signal-Space Separation\n",
    "*Approx. time: 15 m*\n",
    "\n",
    "### Compute the head positions\n",
    "\n",
    "First, we compute the continuous head positions thanks to the coils. These allow us to compensate for the head movements and helps to clean the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract continuous head position indicators (cHPI)\n",
    "chpi_amplitudes = mne.chpi.compute_chpi_amplitudes(raw_bids, t_window = 0.5, t_step_min = 0.1)\n",
    "\n",
    "# Estimate the coils locations\n",
    "chpi_locs = mne.chpi.compute_chpi_locs(raw_bids.info, chpi_amplitudes)\n",
    "\n",
    "# Estimate the head positions\n",
    "head_pos = mne.chpi.compute_head_pos(raw_bids.info, chpi_locs, verbose = True)\n",
    "\n",
    "# Save the head positions\n",
    "path_pos = op.join(path_derivatives.directory, path_derivatives.basename + \".pos\")\n",
    "mne.chpi.write_head_pos(path_pos, head_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Maxwell filter\n",
    "Now, we run a Maxwell filter and a Signal-Space Separation (SSS) to get a signal with reduced environmental noise and head movement compensation. These two techniques are performed together by calling the filter function from MNE. \n",
    "\n",
    "It is important to have detected the bad channels beforehands, as bad sensors impact drastically the filtering of their neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Maxfilter\n",
    "raw_sss = mne.preprocessing.maxwell_filter(\n",
    "    raw_bids, \n",
    "    cross_talk = crosstalk_file,  # MEG crosstalk file\n",
    "    calibration = fine_cal_file,  # MEG fine-calibration file\n",
    "    head_pos = head_pos,  # Head positions\n",
    "    st_duration = 10, # Duration of the buffer (in seconds) \n",
    "    st_correlation = 0.98  # Correlation limit between inner and outer subspaces used to reject overlapping intersecting inner/outer signals during spatiotemporal SSS.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling the data\n",
    "Next, we downsample the data at 200 Hz.\n",
    "\n",
    "The original data from the MEG is sampled at 1000 Hz, with 306 channels. It is possible to encounter an integer overflow in the MNE Python package if the amount of indices in the data matrix reach 2<sup>31</sup> (2 147 483 648). This will be reached after recording more than 1 hour, 56 minutes and 57 seconds of raw recording (7017.92 seconds). To avoid this, we downsample the data at 200 Hz, which also allows to speed up the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting raw_bids to save memory\n",
    "del raw_bids\n",
    "\n",
    "# Downsampling the data at 200 Hz\n",
    "raw_downsampled_sss = raw_sss.resample(sfreq = 200) # Automatic low-pass at 100 Hz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data\n",
    "\n",
    "*Approx. time: 1 m*\n",
    "\n",
    "<span style=\"color:#99cc00\">Creates ``derivatives/Preprocessing/sub-XX/ses-XX/sub-XX_ses-XX_task-track_run-01_sss.fif`` (multiple files).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the filtered data\n",
    "path_downsampled_sss = op.join(path_derivatives.directory, path_derivatives.basename.replace(\"meg\", \"sss\") + \".fif\")\n",
    "raw_downsampled_sss.save(path_downsampled_sss, overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<span style=\"color:#ffcc00\">**Checkpoint #1**</span>\n",
    "\n",
    "<span style=\"color:#ffcc00\">You can safely close the pipeline here and come back later. To start back, you will only need to run the [import](#import) and [path defining](#load) steps.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ica\"></a>\n",
    "## Generate the ICA\n",
    "\n",
    "### Restore the data from a file\n",
    "Only perform this line if you don't want to re-do the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_downsampled_sss = op.join(path_derivatives.directory, path_derivatives.basename.replace(\"meg\", \"sss\") + \".fif\")\n",
    "raw_downsampled_sss = mne.io.read_raw_fif(path_downsampled_sss, preload = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the ICA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Approx. time: 15 m*\n",
    "\n",
    "The ICA tries to detect automatically components in the signal emerging from the data of all the sensors. We use it to detect the heartbeat and the horizontal and vertical eye movements components that we will then reject from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a band-pass filter\n",
    "raw_for_ica = raw_downsampled_sss.copy().filter(l_freq = 0.5, h_freq = 30)\n",
    "\n",
    "# Create the ICA object\n",
    "ica = mne.preprocessing.ICA(  # Create an ICA object\n",
    "    n_components = 20,  # Define how many components we want in our ICA\n",
    "    random_state = 97,   # Define a seed for the randomness generator to get consistent results\n",
    "    method = \"picard\",  # Makes the ICA faster\n",
    "    fit_params = dict(ortho = True, extended = True)\n",
    ")\n",
    "\n",
    "# Fit the ICA to our data\n",
    "ica.fit(raw_for_ica, picks = 'meg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the ICA\n",
    "\n",
    "*Approx. time: 1 s*\n",
    "\n",
    "<span style=\"color:#99cc00\">Creates ``derivatives/Preprocessing/sub-XX/ses-XX/sub-XX_ses-XX_task-track_run-01_ica_solution.fif`` (multiple files).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ica_solution = op.join(path_derivatives.directory, path_derivatives.basename.replace(\"meg\", \"ica_solution\") + \".fif\")\n",
    "ica.save(path_ica_solution, overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<span style=\"color:#ffcc00\">**Checkpoint #2**</span>\n",
    "\n",
    "<span style=\"color:#ffcc00\">You can safely close the pipeline here and come back later. To start back, you will only need to run the [import](#import) and [path defining](#load) steps.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"remove\"></a>\n",
    "## Remove the heartbeat and eye movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the ICA and filtered data from file\n",
    "*Approx. time: 2 m*\n",
    "\n",
    "Only perform this line if you don't want to re-do the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw for ICA data\n",
    "path_downsampled_sss = op.join(path_derivatives.directory, path_derivatives.basename.replace(\"meg\", \"sss\") + \".fif\")\n",
    "raw_downsampled_sss = mne.io.read_raw_fif(path_downsampled_sss, preload = True)\n",
    "raw_for_ica = raw_downsampled_sss.copy().filter(l_freq = 0.5, h_freq = 30)\n",
    "\n",
    "# Load the ICA\n",
    "path_ica_solution = op.join(path_derivatives.directory, path_derivatives.basename.replace(\"meg\", \"ica_solution\") + \".fif\")\n",
    "ica = mne.preprocessing.read_ica(path_ica_solution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ICA\n",
    "The ICA should detect the heartbeat and the eye movements (both vertical and horizontal) among the first components, as they are strong signals that are collected by most, if not all sensors.\n",
    "We can take a look at the output from the ICA two ways: by plotting the time course of the components, or by plotting the scalp field distribution of the components.\n",
    "\n",
    "Here is [a resource](https://labeling.ucsd.edu/tutorial/labels) to learn to identify the components properly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the scalp field distrubution\n",
    "The scalp map of the **heartbeat** will look like that of a very far dipole and so will look almost like a linear gradient. \n",
    "\n",
    "The **vertical eye movements** have a scalp topography that is oriented up and down.\n",
    "\n",
    "The **horizontal eye movements** have a scalp topography that can be modeled as a dipole with left/right with positive values on one side and negative on the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = ica.plot_components()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the time course\n",
    "The **heartbeat** component should look like a regular, strong pulse.\n",
    "\n",
    "The **vertical eye movements** component should show clear spikes relatively frequently due to eye blinks, with stable movements in between, of generally lower amplitude than the other components.\n",
    "\n",
    "The **horizontal eye movements** component should have intervals of relative stability with occasional and very fast transitions to different values due to visual scanning (if the task involves it).\n",
    "\n",
    "Both the vertical and horizontal eye movements should be below 5 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.set_browser_backend(\"qt\")\n",
    "raw_for_ica.load_data()\n",
    "ica.plot_sources(raw_for_ica, show_scrollbars = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check EOG and ECG channels\n",
    "\n",
    "We now need to ensure that the channels for the ECG, VEOG and HEOG are the good ones for our participant. In some participants, the channels can be different from the default ones (HEOG: EOG061, VEOG: EOG062, ECG: ECG063).\n",
    "\n",
    "Here, we plot the channels. If the channels do not look like the good ones, we can use `raw.plot()` to try to find the proper channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.set_browser_backend(\"qt\")\n",
    "\n",
    "# Define the EOG and ECG channels\n",
    "# Check that they match the actual EOG/ECG info!\n",
    "eog_ecg_channels = {\"default\": {\"HEOG\": \"EOG061\", \"VEOG\": \"EOG062\", \"ECG\": \"ECG063\"},\n",
    "                    \"subject_01a\": {\"HEOG\": \"EOG061\", \"VEOG\": \"EOG062\", \"ECG\": \"EEG005\"},\n",
    "                    \"subject_01b\": {\"HEOG\": \"EOG061\", \"VEOG\": \"EOG062\", \"ECG\": \"ECG063\"},\n",
    "                    \"subject_02a\": {\"HEOG\": \"EOG061\", \"VEOG\": \"EOG062\", \"ECG\": \"ECG063\"},\n",
    "                    \"subject_02b\": {\"HEOG\": \"EOG061\", \"VEOG\": \"EOG062\", \"ECG\": \"ECG063\"}}\n",
    "\n",
    "heog = eog_ecg_channels[subject][\"HEOG\"]  # Default: E0G061\n",
    "veog = eog_ecg_channels[subject][\"VEOG\"]  # Default: E0G062\n",
    "ecg = eog_ecg_channels[subject][\"ECG\"]  # Default: ECG063\n",
    "\n",
    "# Check using this line\n",
    "channels_of_interest = raw_for_ica.copy().pick_channels([heog, veog, ecg])\n",
    "channels_of_interest.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the raw data\n",
    "Only use if you think that the channels selected don't look like the good ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_for_ica.plot(overview_mode = \"hidden\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the ICA components matching the EOG and ECG\n",
    "*Approx. time: 5 m*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze EOG/ECG channels\n",
    "heog_indices, heog_scores = ica.find_bads_eog(raw_for_ica, ch_name = heog)\n",
    "veog_indices, veog_scores = ica.find_bads_eog(raw_for_ica, ch_name = veog)\n",
    "ecg_indices, ecg_scores = ica.find_bads_ecg(raw_for_ica, ch_name = ecg)\n",
    "\n",
    "# Reject ICAs containing the EOG and ECG information from all channels\n",
    "ica.exclude = list(set(veog_indices + heog_indices + ecg_indices))\n",
    "print(f\"Rejected ICAS: {str(ica.exclude)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the components matching the EOG and ECG\n",
    "\n",
    "Note: all of the next sub-steps will allow you to take an informed decision whether to keep or change the components to remove. It is not necessary to run them every time, but all inform you as to what type of information you would be removing by selecting some components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the correlation scores for each channel/ICA\n",
    "*Approx. time: 1 s*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 3)\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "plt.setp(axes, xticks=[i for i in range(ica.n_components)], ylim=[0, 1])\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.bar([i for i in range(ica.n_components)], np.abs(heog_scores), color=[\"blue\" if i in heog_indices else \"black\" for i in range(ica.n_components)])\n",
    "plt.title(\"HEOG\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.bar([i for i in range(ica.n_components)], np.abs(veog_scores), color=[\"green\" if i in veog_indices else \"black\" for i in range(ica.n_components)])\n",
    "plt.title(\"VEOG\")\n",
    "\n",
    "plt.sca(axes[2])\n",
    "plt.bar([i for i in range(ica.n_components)], np.abs(ecg_scores), color=[\"red\" if i in ecg_indices else \"black\" for i in range(ica.n_components)])\n",
    "plt.title(\"ECG\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the explained variance of each component\n",
    "This allow to know how much variance in the data is explained by each component, in order to take a decision as to whether remove a component or not. Removing a component with a lot of variance will have more impact on the overall data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitize variances explained by PCA components, so the values sum to 1\n",
    "pca_explained_variances = ica.pca_explained_variance_ / ica.pca_explained_variance_.sum()\n",
    "\n",
    "# Now extract the variances for those components that were used to perform ICA\n",
    "ica_explained_variances = pca_explained_variances[:ica.n_components_]\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xticks([i for i in range(ica.n_components)])\n",
    "axes.set_ylim([0, 1])\n",
    "axes.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "plt.bar([i for i in range(ica.n_components)], ica_explained_variances)\n",
    "\n",
    "for i in range(ica.n_components):\n",
    "        plt.text(i,  # x position\n",
    "                 ica_explained_variances[i] + .05,  # y position\n",
    "                 str(round(ica_explained_variances[i]*100, 2)) + \" %\",  # Text to plot\n",
    "                 ha = 'center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the rejected components to the EOG and ECG channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.set_browser_backend(\"qt\")\n",
    "plot = ica.plot_sources(raw_for_ica, picks=ica.exclude, start=0., stop=30., show_scrollbars=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually reject components\n",
    "If the previous step has not rejected an ICA, or has partially rejected the ICAs, you can manually reject components here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add components\n",
    "ica.exclude += []\n",
    "\n",
    "# Remove components\n",
    "ica.exclude = [comp for comp in ica.exclude if comp not in []]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Superimpose the data before and after removing a specific component\n",
    "*Approx. time: 10 s*\n",
    "\n",
    "Shows what some selected channels will look like after removing a selected component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
    "plot = ica.plot_overlay(\n",
    "    raw_for_ica,  # The original data\n",
    "    exclude = ica.exclude,  # The ICA component to remove\n",
    "    picks = 'meg',   # The channels to display\n",
    "    start = 0.,  # First timestamp of the display window (seconds)\n",
    "    stop = 30.,  # Last timestamp of the display window (seconds)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reject the components\n",
    "*Approx. time: 20 s*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_filtered_ica = ica.apply(raw_for_ica)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the ICA-filtered data\n",
    "*Approx. time: 1 m*\n",
    "\n",
    "<span style=\"color:#99cc00\">Creates ``derivatives/Preprocessing/sub-XX/ses-XX/sub-XX_ses-XX_tast-track_run-01_ica.fif`` (multiple files).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ica = op.join(path_derivatives.directory, path_derivatives.basename.replace(\"meg\", \"ica\") + \".fif\")\n",
    "raw_filtered_ica.save(path_ica, overwrite = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the pictures showing the ICA components\n",
    "*Approx. time: 5 s*\n",
    "\n",
    "<span style=\"color:#99cc00\">Creates the following image files in the folder ``derivatives/Preprocessing/sub-XX/ses-XX/``:\n",
    "\n",
    "* ``sub-XX_ses-XX_task-track_run-01_ecg_scores.png``\n",
    "* ``sub-XX_ses-XX_task-track_run-01_heog_scores.png``\n",
    "* ``sub-XX_ses-XX_task-track_run-01_veog_scores.png``\n",
    "* ``sub-XX_ses-XX_task-track_run-01_ica_all_components.png``\n",
    "* ``sub-XX_ses-XX_task-track_run-01_ica_rejected_components.png``\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All components\n",
    "f1 = ica.plot_components()\n",
    "path_ica_all_components = op.join(path_derivatives.directory, path_derivatives.basename.replace(\"meg\", \"ica_all_components\") + \".png\")\n",
    "f1.savefig(path_ica_all_components, dpi=300)\n",
    "\n",
    "# Rejected components\n",
    "f2 = ica.plot_components(picks=ica.exclude)\n",
    "path_ica_rejected_components = op.join(path_derivatives.directory, path_derivatives.basename.replace(\"meg\", \"ica_rejected_components\") + \".png\")\n",
    "f2.savefig(path_ica_rejected_components, dpi=300)\n",
    "\n",
    "# HEOG channel\n",
    "f3 = ica.plot_scores(heog_scores, show=False)\n",
    "path_ica_heog = op.join(path_derivatives.directory, path_derivatives.basename.replace(\"meg\", \"heog_scores\") + \".png\")\n",
    "f3.savefig(path_ica_heog, dpi=300)\n",
    "\n",
    "# VEOG channel\n",
    "f4 = ica.plot_scores(veog_scores, show=False)\n",
    "path_ica_veog = op.join(path_derivatives.directory, path_derivatives.basename.replace(\"meg\", \"veog_scores\") + \".png\")\n",
    "f4.savefig(path_ica_veog, dpi=300)\n",
    "\n",
    "# ECG channel\n",
    "f5 = ica.plot_scores(ecg_scores, show=False)\n",
    "path_ica_ecg = op.join(path_derivatives.directory, path_derivatives.basename.replace(\"meg\", \"ecg_scores\") + \".png\")\n",
    "f5.savefig(path_ica_ecg, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save ICA in eelbrain format\n",
    "If you want to perform a mTRF analysis, you may want to save your data for eelbrain.\n",
    "\n",
    "<span style=\"color:#99cc00\">Creates ``eelbrain/meg/sub-XX/sub-XX_track-raw.fif`` (multiple files).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_eelbrain_meg = op.join(path_output, 'eelbrain', 'meg')\n",
    "path_eelbrain = op.join(path_eelbrain_meg, subject_eelbrain_code, subject_eelbrain_code + \"_track-raw.fif\")\n",
    "raw_eelbrain = raw_filtered_ica.copy().pick(['meg','stim', 'misc', 'chpi'])  # We remove the EEG channels (in the case they exist) as they conflict with eelbrain\n",
    "raw_eelbrain.save(path_eelbrain, overwrite = True, verbose = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a look at the clear data\n",
    "We can plot the raw data before and after ICA to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.set_browser_backend(\"qt\")  # Put \"matplotlib\" if you prefer to get it inline or \"qt\" if you want it in a window\n",
    "plot = raw_downsampled_sss.plot(show_scrollbars = False, overview_mode = \"hidden\")\n",
    "plot = raw_filtered_ica.plot(show_scrollbars = False, overview_mode = \"hidden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<span style=\"color:#ffcc00\">**Checkpoint #3**</span>\n",
    "\n",
    "<span style=\"color:#ffcc00\">You can safely close the pipeline here and come back later. To start back, you will only need to run the [import](#import) and [path defining](#load) steps.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"empty\"></a>\n",
    "## Pre-process the empty-room recording"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best empty room measurement\n",
    "\n",
    "*Approx. time: 1 s*\n",
    "\n",
    "We first need to find the temporally closest empty room from the folder `lab/MEG_EXPERIMENTS/EMPTYROOM/DATA/`.\n",
    "\n",
    "The code below prints the recording date of the current subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = mne.io.read_info(raw_fif_name, verbose = False)\n",
    "date_measurement = info[\"meas_date\"].replace(tzinfo = None)\n",
    "print(f\"Measurement date: {date_measurement.strftime('%A %d %B %Y %H:%M:%S')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below automatically finds the closest empty room recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the recordings folders\n",
    "path_empty_room = op.join('/export/home', user, 'lab/MEG_EXPERIMENTS/EMPTYROOM/DATA/')\n",
    "subfolders_empty_room = os.listdir(path_empty_room)\n",
    "subfolders_empty_room.remove(\"other\")\n",
    "\n",
    "empty_room_recordings = []\n",
    "for subfolder in subfolders_empty_room:\n",
    "    subfolder_recordings = os.listdir(op.join(path_empty_room, subfolder))\n",
    "    for recording in subfolder_recordings:\n",
    "        if op.isdir(op.join(path_empty_room, subfolder, recording)):\n",
    "            empty_room_recordings.append(subfolder + \"/\" + recording)\n",
    "\n",
    "# Find the smallest difference of days\n",
    "smallest_diff = None\n",
    "selected_recording = None\n",
    "for folder in empty_room_recordings:\n",
    "    recording = folder.split(\"/\")[1]\n",
    "    if recording != \".directory\":\n",
    "        date_empty_room = dt(int(recording[0:2]) + 2000, int(recording[2:4]), int(recording[4:6]))  # Turn the folder name into a date\n",
    "        day_diff = date_measurement - date_empty_room\n",
    "        if selected_recording is None or abs(smallest_diff) > abs(day_diff):  # If we find a smaller difference of days than the previous, we set it\n",
    "            smallest_diff = day_diff\n",
    "            selected_recording = folder\n",
    "\n",
    "# Print the selected empty room\n",
    "folder_name = selected_recording.split(\"/\")[1]\n",
    "empty_room_date = dt(int(folder_name[0:2]) + 2000, int(folder_name[2:4]), int(folder_name[4:6])).strftime(\"%A %d %B %Y\")\n",
    "if smallest_diff.days > 0:\n",
    "    print(f\"Selected empty room recording: {selected_recording}, recorded {empty_room_date} ({smallest_diff.days} day(s) before the subject recording).\")\n",
    "elif smallest_diff.days < 0:\n",
    "    print(f\"Selected empty room recording: {selected_recording}, recorded {empty_room_date} ({abs(smallest_diff.days)} day(s) after the subject recording).\")\n",
    "else:\n",
    "    print(f\"Selected empty room recording: {selected_recording}, recorded {empty_room_date} (the same day of the subject recording).\")\n",
    "\n",
    "path_selected_er = op.join(path_empty_room, selected_recording)\n",
    "path_empty_room_fif = op.join(path_selected_er, os.listdir(path_selected_er)[0])\n",
    "print(f\"Path: {path_empty_room_fif}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the empty room recording in BIDS format\n",
    "\n",
    "*Approx. time: 15 s*\n",
    "\n",
    "<span style=\"color:#99cc00\">Creates the following files in the ``MEGtrain/sub-XX/ses-XX/`` folder:</span>\n",
    "\n",
    "* ``meg/sub-XX_ses-XX_task-emptyroom_run-01_channels.tsv``\n",
    "* ``meg/sub-XX_ses-XX_task-emptyroom_run-01_meg.json``\n",
    "* ``meg/sub-XX_ses-XX_task-emptyroom_run-01_split-XX_meg.fif``</span>\n",
    "\n",
    "<span style=\"color:#99cc00\">The files ``sub-XX_ses-XX_scans.tsv`` and ``meg/sub-XX_ses-XX_coordsystem.json`` are also updated.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the FIF file\n",
    "empty_room_fif = mne.io.read_raw_fif(path_empty_room_fif, preload = False)\n",
    "\n",
    "# We remove any digitization point that could be saved from a previous session, so we use a proper empty room\n",
    "empty_room_fif.set_montage(None)\n",
    "\n",
    "# Write the BIDS files\n",
    "path_bids_empty_room = BIDSPath(\n",
    "    subject = subject_number, \n",
    "    session = subject_session, \n",
    "    task = 'emptyroom', \n",
    "    run = \"01\",\n",
    "    datatype = \"meg\", \n",
    "    root = path_bids)\n",
    "\n",
    "write_raw_bids(raw = empty_room_fif, bids_path = path_bids_empty_room, overwrite = True)\n",
    "\n",
    "# We delete the empty_room_fif to free some memory\n",
    "del empty_room_fif"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad channel detection\n",
    "\n",
    "#### Automatic detection\n",
    "*Approx. time: 5 s*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the BIDS data\n",
    "raw_bids_empty_room = read_raw_bids(path_bids_empty_room)\n",
    "\n",
    "# Define the \"bads\" channels to being empty\n",
    "raw_bids_empty_room.info['bads'] = []\n",
    "\n",
    "# Run the automatic detection\n",
    "auto_noisy_channels_er, auto_flat_channels_er, auto_scores_er = mne.preprocessing.find_bad_channels_maxwell(\n",
    "    raw_bids_empty_room,  # Raw data to process\n",
    "    cross_talk = crosstalk_file, \n",
    "    calibration = fine_cal_file,\n",
    "    return_scores = True, \n",
    "    coord_frame = \"meg\",  # Necessary for empty room recordings\n",
    "    duration = 30,  # Duration of each window (in seconds)\n",
    "    min_count = 10,  # Number of window appearances to be counted as bad channel\n",
    "    verbose = True,\n",
    "    )\n",
    "\n",
    "print(f\"Flat channels: {str(auto_flat_channels_er)}\")\n",
    "print(f\"Bad channels: {str(auto_noisy_channels_er)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_bids_empty_room.plot(lowpass = 30, highpass = 1, overview_mode = \"hidden\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually writing the bad channels\n",
    "bad_channels_er_manual = {\"subject_06a\": [\"MEG1122\", \"MEG2623\"],\n",
    "                          \"subject_06b\": [],\n",
    "                          \"subject_07a\": [\"MEG1122\", \"MEG2623\"],\n",
    "                          \"subject_07b\": [\"MEG0312\"]}\n",
    "\n",
    "raw_bids_empty_room.info[\"bads\"] = bad_channels_er_manual[subject]\n",
    "\n",
    "# Combining the automatically found bad and flat channels to the manual ones\n",
    "raw_bids_empty_room.info[\"bads\"] += auto_noisy_channels_er + auto_flat_channels_er\n",
    "\n",
    "print(\"Bad channels: \" + str(raw_bids_empty_room.info[\"bads\"]))\n",
    "\n",
    "# Once the bad channels are defined, we save them in the file itself\n",
    "mark_channels(path_bids_empty_room, ch_names = raw_bids_empty_room.info['bads'], status = \"bad\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxwell filtering and Signal-Space Separation\n",
    "*Approx. time: 1 m*\n",
    "\n",
    "<span style=\"color:#99cc00\">Creates ``derivatives/Preprocessing/sub-XX/ses-XX/sub-XX_ses-XX_task-emptyroom_run-01_sss.fif``.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the saving path\n",
    "path_derivatives_er = BIDSPath(\n",
    "    subject = subject_number, \n",
    "    session = subject_session, \n",
    "    task = 'emptyroom', \n",
    "    run = \"01\",\n",
    "    suffix = \"meg\",\n",
    "    root = path_preprocessing\n",
    ")\n",
    "\n",
    "# Run the Maxfilter\n",
    "raw_er_sss = mne.preprocessing.maxwell_filter(\n",
    "    raw_bids_empty_room, \n",
    "    cross_talk = crosstalk_file, \n",
    "    calibration = fine_cal_file,\n",
    "    coord_frame = \"meg\",  # Necessary for empty room recordings\n",
    "    st_duration = 10,  # Duration of the buffer (in seconds) \n",
    "    st_correlation = 0.98  # Correlation limit between inner and outer subspaces used to reject overlapping intersecting inner/outer signals during spatiotemporal SSS.\n",
    ")\n",
    "\n",
    "# Downsampling the data\n",
    "raw_er_downsampled_sss = raw_er_sss.resample(sfreq = 200)  # Automatic low-pass at 100 Hz\n",
    "\n",
    "# Saving the downsampled data\n",
    "path_downsampled_sss_er = op.join(path_derivatives_er.directory, path_derivatives_er.basename.replace(\"meg\", \"sss\") + \".fif\")\n",
    "raw_er_downsampled_sss.save(path_downsampled_sss_er, overwrite = True, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save ICA in eelbrain format\n",
    "\n",
    "<span style=\"color:#99cc00\">Creates ``eelbrain/meg/sub-XX/sub-XX_emptyroom-raw.fif`` (multiple files).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_eelbrain_meg = op.join(path_output, 'eelbrain', 'meg')\n",
    "path_eelbrain_er = op.join(path_eelbrain_meg, subject_eelbrain_code, subject_eelbrain_code + \"_emptyroom-raw.fif\")\n",
    "raw_er_eelbrain = raw_er_downsampled_sss.copy().pick(['meg','stim'])  # We remove the EEG channels (in the case they exist) as they conflict with eelbrain\n",
    "raw_er_eelbrain.save(path_eelbrain_er, overwrite = True, verbose = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"freesurfer\"></a>\n",
    "## Cortical reconstruction with Freesurfer\n",
    "*Approx. time: 8 h*\n",
    "\n",
    "The next step is to get a reconstruction of the cortex using Freesurfer. This step is by far the longest. **This step must be ran on cajal01**.\n",
    "\n",
    "From Citrix, open **Terminator** (Linux Apps). Once on your virtual desktop, connect to cajal03: `vglconnect USERNAME@cajal01 -Y`, and enter your password.\n",
    "\n",
    "Note: to paste code in bash, you have by default to press Ctrl+Shift+V. You can change this in the parameters of the console, in the tab named \"Keybindings\" by something less stupid, like Ctrl+V."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Freesurfer\n",
    "`module load freesurfer/6.0.0`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the necessary Freesurfer variables\n",
    "Output directory:\n",
    "\n",
    "`export SUBJECTS_DIR=~/public/MEGtrain/eelbrain/mri`\n",
    "\n",
    "Subject name (will be the name of the folder created in the output directory)\n",
    "\n",
    "`export SUBJECT=sub-01`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the MRI data path and run the code\n",
    "\n",
    "<span style=\"color:#99cc00\">This step creates ``eelbrain/mri/sub-XX/`` and all its contents (subfolders ``label``, ``mri``, ``scripts``, ``stats``, ``surf``, ``tmp``, ``touch`` and ``trash``). ``tmp`` and ``trash`` folders are empty.</span>\n",
    "\n",
    "**Important note:** Bash is a very old and stupidly designed language that is scared by blank spaces. One of the folders in the full path to the T1 is named `CRANEO_FUNCIONAL - 1`, with two spaces around the hyphen. While some command lines allow to bypass this by putting backslashes before each space (`CRANEO_FUNCIONAL\\ -\\ 1`) or putting simple or double quotes around the path (`\"CRANEO_FUNCIONAL - 1\"`), the Freesurfer command line inexplicably doesn't manage to understand that the path contains spaces. As a workaround, **delete the spaces from the path** - you cannot do that from the lab folder, but the T1s have been copied to `~/public/MEGtrain/eelbrain/mri/t1`. There, you can rename the folder `CRANEO_FUNCIONAL - 1` by `CRANEO_FUNCIONAL-1`.\n",
    "\n",
    "Once you did that, you have many possibilities:\n",
    "\n",
    "* Get into the path containing the data\n",
    "\n",
    "    `cd \"~/public/MEGtrain/eelbrain/mri/t1/EXPERIMENT_NAME/CRANEO_FUNCIONAL-1/t1_mprage_sag_p2_1iso_MGH_6\"`\n",
    "\n",
    "    In that case, run the code that way:\n",
    "\n",
    "    `nice recon-all -s $SUBJECT -i IM-0004-0001.dcm -autorecon-all -mail YOURMAILADDRESS`\n",
    "\n",
    "* Put the whole path in the code line (this line will run independently of your current working directory):\n",
    "\n",
    "    `nice recon-all -s $SUBJECT -i ~/public/MEGtrain/eelbrain/mri/t1/EXPERIMENT_NAME/CRANEO_FUNCIONAL-1/t1_mprage_sag_p2_1iso_MGH_6/IM-0004-0001.dcm -autorecon-all -mail YOURMAILADDRESS`\n",
    "\n",
    "Note: You can run multiple jobs in parallel by pressing Ctrl+Shift+T on Terminator: this will open a new tab where you can run another participant. However, keep in mind the resources of the cluster are shared between the BCBL members. You can check how busy the cluster currently is by typing `top`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"coregistration\"></a>\n",
    "## Co-registration (from bash)\n",
    "*Approx. time: 20 m*\n",
    "\n",
    "The final step is to perform a co-registration between the MRI and the MEG space. We will once again need to use Freesurfer, on cajal01."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment\n",
    "    module load freesurfer/6.0.0\n",
    "    module load python/python3.6\n",
    "    source activate eelbrain\n",
    "    vglrun ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your variables\n",
    "You are now in iPython. Start by importing the modules you will need:\n",
    "\n",
    "    import mne\n",
    "    import os\n",
    "\n",
    "Then, define your variables:\n",
    "\n",
    "    user = USERNAME\n",
    "    path_output = os.path.join('/export/home', user, 'public/MEGtrain/')\n",
    "    subjects_dir = os.path.join(path_output, 'eelbrain/mri/')\n",
    "    os.environ[\"SUBJECTS_DIR\"] = subjects_dir\n",
    "    subject = 'sub-01'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the watershed folder\n",
    "*Approx. time: 8 m*\n",
    "\n",
    "<span style=\"color:#99cc00\">This step creates ``eelbrain/mri/sub-XX/bem`` and all its contents:\n",
    "\n",
    "* ``brain.surf``\n",
    "* ``inner_skull.surf``\n",
    "* ``outer_skin.surf``\n",
    "* ``outer_skull.surf``\n",
    "* ``sub-XX-head.fif``\n",
    "* ``watershed/sub-XX_brain_surface``\n",
    "* ``watershed/sub-XX_inner_skull_surface``\n",
    "* ``watershed/sub-XX_outer_skin_surface``\n",
    "* ``watershed/sub-XX_outer_skull_surface``\n",
    "* ``watershed/ws.mgz``\n",
    "\n",
    "</span>\n",
    "\n",
    "``mne.bem.make_watershed_bem(subject = subject, overwrite = True)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the head-dense.fif file\n",
    "*Approx. time: 1 m*\n",
    "\n",
    "<span style=\"color:#99cc00\">This step creates the following files in the ``eelbrain/mri/sub-XX/bem`` folder:\n",
    "\n",
    "* ``sub-XX-head-dense.fif``\n",
    "* ``sub-XX-head-medium.fif``\n",
    "* ``sub-XX-head-sparse.fif``\n",
    "\n",
    "</span>\n",
    "\n",
    "``mne.bem.make_scalp_surfaces(subject = subject, overwrite = True, force = True)``"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the hemispheres\n",
    "You can now visualize each hemisphere within the head.\n",
    "\n",
    "    Brain = mne.viz.get_brain_class()  # Create a Brain object\n",
    "    brain = Brain(subject, \n",
    "                  hemi = 'both',  # The hemisphere to visualize (`lh`, `rh`, `both`, or `split`)\n",
    "                  subjects_dir = subjects_dir, \n",
    "                  size = (800, 600)  # The size of the window in pixels\n",
    "    )\n",
    "    brain.add_annotation('aparc.a2009s', borders = False)\n",
    "    brain.add_head(dense = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the co-registration\n",
    "*Approx. time: 5 m per session*\n",
    "\n",
    "<span style=\"color:#99cc00\">This step creates the following files:\n",
    "\n",
    "* ``eelbrain/mri/sub-XX/bem/sub-XX-fiducials.fif``\n",
    "* ``bids/sub-XX/ses-XX/sub-XX_ses-XX_task-track_run-01_trans.fif``\n",
    "\n",
    "</span>\n",
    "\n",
    "Finally, you can perform the co-registration with the head image.\n",
    "\n",
    "To create the transform from head to MRI coordinate frame:\n",
    "\n",
    "    mne.gui.coregistration(subject = subject, subjects_dir = subjects_dir)\n",
    "\n",
    "This opens a GUI allowing you to align the MEG Polhemus coordinates to the MRI.\n",
    "\n",
    "* Define the fiducials and lock them (the nasion should be centered between the eyes, while the RPA and LPA should be at the entrance of the ear, just touching the tragus).\n",
    "* Save the fiducials.\n",
    "\n",
    "**For each session:**\n",
    "* Under Info source with digitization, select the path to the first split fif file of the session, e.g. `/export/home/USERNAME/public/MEGtrain/bids/sub-06/ses-01/meg/sub-06_ses-01_task-track_run-01_split-01_meg.fif` (it shouldn't matter to run the co-registration on the raw file or the pre-processed file as the coordinates are left untouched).\n",
    "* Align the MEG dots with the MRI head using the translation and rotation parameters (on the left - generally a x-rotation of at least 30掳 is necessary).\n",
    "* Save the HEAD <> MRI transform under `/MEGtrain/bids/sub-XX/ses-XX/sub-XX_ses-XX_task-track_run-01_trans.fif`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"coregistrationvsc\"></a>\n",
    "## Co-registration (run from Visual Studio Code, works partially)\n",
    "*Approx. time: 20 m*\n",
    "\n",
    "The final step is to perform a co-registration between the MRI and the MEG space. \n",
    "\n",
    "Run this on **cajal01**. \n",
    "\n",
    "If you run this from Visual Studio Code, you may need to install the pre-version for Pylance and Python (by opening Extensions in the lateral menu). Beware that doing so will mean that you will have to reinstall Python on cajal03."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the environment\n",
    "Load **Python** using `module load python`.\n",
    "\n",
    "Load **Freesurfer** using `module load freesurfer/6.0.0`.\n",
    "\n",
    "Run **Visual Studio Code** using `code`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your variables (works in VSC)\n",
    "If you closed this file since the pre-processing, re-run the two first code blocks ([Import sections](#setup) and [Locate the subject data and define output path](#load) - enter the subject you want to process there). Then, prepare the variables needed for the co-registration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_subject = \"sub-\" + str(subject_number)\n",
    "subjects_dir = op.join(path_output, 'eelbrain/mri/')\n",
    "os.environ[\"SUBJECTS_DIR\"] = subjects_dir\n",
    "os.environ[\"MESA_GL_VERSION_OVERRIDE\"] = \"3.3\"\n",
    "os.environ[\"MNE_3D_OPTION_MULTI_SAMPLES\"] = \"1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the watershed folder (works in VSC)\n",
    "*Approx. time: 8 m*\n",
    "\n",
    "<span style=\"color:#99cc00\">Creates: ``eelbrain/mri/sub-XX/bem`` folder, with:\n",
    "* ``brain.surf``\n",
    "* ``inner_skull.surf``\n",
    "* ``outer_skin.surf``\n",
    "* ``outer_skull.surf``\n",
    "* ``sub-XX-head.fif``\n",
    "* ``watershed/sub-XX_brain_surface``\n",
    "* ``watershed/sub-XX_inner_skull_surface``\n",
    "* ``watershed/sub-XX_outer_skin_surface``\n",
    "* ``watershed/sub-XX_outer_skull_surface``\n",
    "* ``watershed/ws.mgz``\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.bem.make_watershed_bem(subject = fs_subject, overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the head-dense.fif file (works in VSC)\n",
    "*Approx. time: 1 min*\n",
    "\n",
    "<span style=\"color:#99cc00\">This step creates the following files in the ``eelbrain/mri/sub-XX/bem`` folder:\n",
    "\n",
    "* ``sub-XX-head-dense.fif``\n",
    "* ``sub-XX-head-medium.fif``\n",
    "* ``sub-XX-head-sparse.fif``\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.bem.make_scalp_surfaces(subject = fs_subject, overwrite = True, force = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the hemispheres (does NOT work in VSC)\n",
    "You can now visualize each hemisphere within the head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brain = mne.viz.get_brain_class()  # Create a Brain object\n",
    "brain = Brain(fs_subject, \n",
    "        hemi = 'both',  # The hemisphere to visualize (`lh`, `rh`, `both`, or `split`)\n",
    "        subjects_dir = subjects_dir, \n",
    "        size = (800, 600)  # The size of the window in pixels\n",
    ")\n",
    "brain.add_annotation('aparc.a2009s', borders = False)\n",
    "brain.add_head(dense = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the co-registration (does NOT work in VSC)\n",
    "*Approx. time: 5 m per session*\n",
    "\n",
    "<span style=\"color:#99cc00\">This step creates the following files:\n",
    "\n",
    "* ``eelbrain/mri/sub-XX/bem/sub-XX-fiducials.fif``\n",
    "* ``bids/sub-XX/ses-XX/sub-XX_ses-XX_task-track_run-01_trans.fif``\n",
    "\n",
    "</span>\n",
    "\n",
    "Finally, you can perform the co-registration with the head image.\n",
    "\n",
    "To create the transform from head to MRI coordinate frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.gui.coregistration(subject = fs_subject, subjects_dir = subjects_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This opens a GUI allowing you to align the MEG Polhemus coordinates to the MRI.\n",
    "\n",
    "* Define the fiducials and lock them (the nasion should be centered between the eyes, while the RPA and LPA should be at the entrance of the ear, just touching the tragus).\n",
    "* Save the fiducials.\n",
    "\n",
    "**For each session:**\n",
    "* Under Info source with digitization, select the path to the first split fif file of the session, e.g. `/export/home/USERNAME/public/MEGtrain/bids/sub-06/ses-01/meg/sub-06_ses-01_task-track_run-01_split-01_meg.fif` (it shouldn't matter to run the co-registration on the raw file or the pre-processed file as the coordinates are left untouched).\n",
    "* Align the MEG dots with the MRI head using the translation and rotation parameters (on the left - generally a x-rotation of at least 30掳 is necessary).\n",
    "* Save the HEAD <> MRI transform under `/MEGtrain/bids/sub-XX/ses-XX/sub-XX_ses-XX_task-track_run-01_trans.fif`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eelbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
